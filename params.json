{"name":"Wellsfargocomp","tagline":"Analytics Competition Wells Fargo","body":"# Introduction\r\n\r\nDuring all the competition the objective of the team was to do a ranking of the best evaluated and well commented banks from the four banks in the data base that was gave to us. Since the beginning of the analysis was noticed that the type of social media was described in the data frame, and was divided in two: Facebook and Twitter.\r\n\r\nBoth social media had short info about: BankA, BankB, BankC and bankD. And was our job to discriminate if they were positive or negative. So the analysis that was choose to do over the data frame was basically to discovery if the Facebook comments or tweets were good or not.\r\n\r\n***\r\n\r\n\r\n## How the analysis work?\r\n\r\nWhat we done for evaluating if the users comments were good or bad was just comparing if words used in those comments were good or bad, based on that we could generate a score, and this score based in a average determinated by us, would classify the comment as: Very positive, average and very negative. \r\n\r\nObviously the commentaries that had very good or very bad grades would be the ones that would call more of all attention. But thinking that our code may have failures in this kind of approach we decided to do a human evaluation of the code by taking a small part of the data frame and evaluate them personally, and see if our evaluation would match with the result that the code was giving to us.\r\n\r\nSee how the code works bellow.\r\n\r\n      score.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n      {\r\n        require(plyr)\r\n        require(stringr)\r\n  \r\n        # we got a vector of sentences. plyr will handle a list\r\n        # or a vector as an \"l\" for us\r\n        # we want a simple array (\"a\") of scores back, so we use \r\n        # \"l\" + \"a\" + \"ply\" = \"laply\":\r\n        scores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n    \r\n       # clean up sentences with R's regex-driven global substitute, gsub():\r\n        sentence = gsub('[[:punct:]]', '', sentence)\r\n        sentence = gsub('[[:cntrl:]]', '', sentence)\r\n        sentence = gsub('\\\\d+', '', sentence)\r\n        # and convert to lower case:\r\n        sentence = tolower(sentence)\r\n    \r\n         # split into words. str_split is in the stringr package\r\n         word.list = str_split(sentence, '\\\\s+')\r\n         # sometimes a list() is one level of hierarchy too much\r\n         words = unlist(word.list)\r\n    \r\n         # compare our words to the dictionaries of positive & negative terms\r\n         pos.matches = match(words, pos.words)\r\n         neg.matches = match(words, neg.words)\r\n    \r\n         # match() returns the position of the matched term or NA\r\n         # we just want a TRUE/FALSE:\r\n         pos.matches = !is.na(pos.matches)\r\n         neg.matches = !is.na(neg.matches)\r\n    \r\n         # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():\r\n         score = sum(pos.matches) - sum(neg.matches)\r\n    \r\n         return(score)\r\n       }, pos.words, neg.words, .progress=.progress )\r\n  \r\n        scores.df = data.frame(score=scores, text=sentences)\r\n        return(scores.df)\r\n     }\r\n\r\n\r\n***\r\n\r\n\r\n## Approach and Methodology\r\n\r\nSeveral initial approaches were tried with the body of consumer content; the most\r\nmeaningful analysis came from comparative methods. The specific procedures we used to obtain\r\nour results are detailed herein.\r\n\r\nIn the coding process, a random sample (~10,000 posts) of the consumer content was\r\ndivided into 5 categories: content mentioning BankA(1), BankB(2), BankC(3), BankD(4), or\r\nnone of the four banks(0). By using a sentiment analysis patch in the code, content was\r\ncategorized as positive or negative, and each of the 5 categories was then scored according to\r\ntheir amount of positive vs. negative content. This gives a rough idea of how consumers\r\nperceive each bank on a large scale.\r\n\r\nFor a deeper analysis, a semi­random sample of 80 content posts was taken and given\r\nextra classifications according to topic, cause of post, and audience. This sample was obtained by\r\ntaking a random sample of 100 content posts from the first four categories, keeping with the\r\ncomparative method used so far, and then determining which posts out of the 100 were\r\nmeaningful. The information gleaned from this analysis, combined with our coded analysis,\r\nprovided us with a cohesive narrative regarding why people post, what they post about, and who\r\ntheir audience is, and how each bank is regarded comparatively.\r\n\r\n***\r\n\r\n\r\n## The graphics\r\n\r\n\r\nThe graphics that were created by the R code are a visual representation of a sample of 10,000 comments of the data frame. And were made 2 kind of graphics: One that is showing the overall of the comment scores for the each one o the banks, and another one for each one of the social networks. In the bank’s graphics you can see that they are shown as 0, 1, 2, 3 and 4.\r\n\r\nThe 0 are for those comments that did not have any bank informed or have multiple banks commented on. The 1 is the bankA, 2 the bankB, 3 the bankC and at last 4 the bankD.\r\n(You can see the graphics that were generated bellow. And the code that created those.)\r\n\r\n    meanscore = tapply(scores$score, scores$mediatype, mean)\r\n    df.plot = data.frame(mediatype=names(meanscore), meanscore=meanscore)\r\n    df.plot$mediatypes <- reorder(df.plot$mediatype, df.plot$meanscore)\r\n\r\n    ggplot(df.plot, aes(x = factor(mediatypes), y = meanscore, fill=mediatypes)) +\r\n      geom_bar(stat=\"identity\") +\r\n      scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n      labs(title = \"Average Sentiment Score\") + \r\n      xlab('Media Type') + ylab('Average Score')\r\n\r\n    #Avarage score for each bank\r\n    meanscore.bank = tapply(scores$score, scores$bank, mean)\r\n    df.plot.bank = data.frame(bank=names(meanscore.bank), meanscore.bank=meanscore.bank)\r\n    df.plot.bank$bank <- reorder(df.plot.bank$bank, df.plot.bank$meanscore)\r\n\r\n    ggplot(df.plot.bank, aes(x = factor(bank), y = meanscore.bank, fill=bank)) +\r\n      geom_bar(stat=\"identity\") +\r\n      scale_fill_manual(values=cols2[order(df.plot.bank$meanscore)]) +\r\n      labs(title = \"Average Sentiment Score For Each Bank\") + \r\n      xlab('Banks') + ylab('Average Score')\r\n\r\n    # barplot of average very positive\r\n\r\n    mediatype_pos = ddply(scores, .(mediatype), summarise, mean_pos=mean(very.pos))\r\n    mediatype_pos$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_pos$mean_pos)\r\n\r\n    ggplot(mediatype_pos, aes(x = factor(mediatypes), y = mediatype_pos$mean_pos, fill=mediatypes)) +\r\n      geom_bar(stat=\"identity\") +\r\n      scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n      labs(title = \"Average Very Positive Sentiment Score\") + \r\n      xlab('Media Type') + ylab('Average Score')\r\n\r\n    #Avarage very positive score for each bank\r\n    #TODO\r\n    banks_pos = ddply(scores, .(bank), summarise, mean_pos=mean(very.pos))\r\n    banks_pos$bank <- reorder(banks_pos$bank, banks_pos$mean_pos)\r\n\r\n    ggplot(banks_pos, aes(x = factor(bank), y = banks_pos$mean_pos, fill=bank)) +\r\n      geom_bar(stat=\"identity\") +\r\n      scale_fill_manual(values=cols2[order(df.plot.bank$meanscore.bank)]) +\r\n      labs(title = \"Average Very Positive Sentiment Score For Each Bank\") + \r\n       xlab('Banks') + ylab('Average Score')\r\n\r\n\r\n\r\n    #Negative Words avarage\r\n    mediatype_neg = ddply(scores, .(mediatype), summarise, mean_pos=mean(very.neg))\r\n    mediatype_neg$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_pos$mean_pos)\r\n\r\n    ggplot(mediatype_pos, aes(x = factor(mediatypes), y = mediatype_neg$mean_pos, fill=mediatypes)) +\r\n      geom_bar(stat=\"identity\") +\r\n      scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n      labs(title = \"Average Very Negative Sentiment Score\") + \r\n      xlab('Media Type') + ylab('Average Score')\r\n\r\n\r\n    # Negative words avarage for each bank\r\n    banks_neg = ddply(scores, .(bank), summarise, mean_pos=mean(very.neg))\r\n    banks_neg$bank <- reorder(banks_neg$bank, banks_neg$mean_pos)\r\n\r\n    ggplot(banks_neg, aes(x = factor(bank), y = banks_neg$mean_pos, fill=bank)) +\r\n      geom_bar(stat=\"identity\") +\r\n      scale_fill_manual(values=cols2[order(df.plot.bank$meanscore.bank)]) +\r\n      labs(title = \"Average Very Negative Sentiment Score For Each Bank\") + \r\n      xlab('Banks') + ylab('Average Score')\r\n![Graphic 01](http://carvalhodeoliveiral.students.cofc.edu/pictures/Rplot05.png)\r\n![Graphic 02](http://carvalhodeoliveiral.students.cofc.edu/pictures/Rplot06.png)\r\n![Graphic 03](http://carvalhodeoliveiral.students.cofc.edu/pictures/Rplot08.png)\r\n\r\nAlso there were made 4 word clouds for each one of the banks with the words that appeared the most in the comments of each of the banks.\r\n(See the word clouds and the code bellow.)\r\n\r\n![wordcloud 01](http://carvalhodeoliveiral.students.cofc.edu/pictures/Rplot.png)\r\n![wordcloud 02](http://carvalhodeoliveiral.students.cofc.edu/pictures/Rplot02.png)\r\n\r\n***\r\n\r\n\r\n## The analysis could be better?\r\n\r\nThis kind of analysis that was used in this project was really simple if you look at it. It was a simple cross data frame validation. If we saw any words that we define as bad, give the code a bad score, if we see a word that we define as good give the comment a good score. We are just looking inside every comment to see if there is bad or good words.\r\n\r\nBut today there is better ways to make this kind of sentiment analysis. And the way to do that is evaluating the comments semantically and not just syntactically. Using some libraries for R or Python we could have done some semantic approach of the comments looking for to see if the whole comment is or not good. \r\nFor example:\r\n\r\n\t* If one of the comments was: “The bankA is not a good bank”\r\n\r\nOur current code would probably treat that as an irrelevant comment, since you have a negative word (not) and a positive word (good), but if we have used some libraries that treated the comments and words as vectors, the good analysis would give us back a bigger vector that would related the word “not” and the word “good”, and it would understand that the combination of those two is a negative thing.\r\n***\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}